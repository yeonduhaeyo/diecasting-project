import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, recall_score, make_scorer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import optuna

# -------------------------------
# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# -------------------------------
df = pd.read_csv("C:\\Users\\USER\\Desktop\\diecasting-project\\data\\data1.csv")
X = df.drop(columns="passorfail")
y = df["passorfail"]

# -------------------------------
# 2. ì»¬ëŸ¼ íƒ€ì… ë¶„ë¦¬
# -------------------------------
cat_cols = X.select_dtypes(include=["object"]).columns.tolist()
num_cols = X.select_dtypes(include=[np.number]).columns.tolist()

print("Categorical:", cat_cols)
print("Numerical:", num_cols)

# -------------------------------
# 3. ì „ì²˜ë¦¬ê¸° ì •ì˜
# -------------------------------
preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
        ("num", StandardScaler(), num_cols)
    ]
)

# -------------------------------
# 4. Optuna ëª©ì  í•¨ìˆ˜ ì •ì˜
# -------------------------------
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

def objective(trial):
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë²”ìœ„ ì •ì˜
    C = trial.suggest_float("C", 1e-3, 10.0, log=True)   # ê·œì œ ê°•ë„
    penalty = trial.suggest_categorical("penalty", ["l1", "l2"])  # ê·œì œ ë°©ì‹
    solver = "liblinear" if penalty == "l1" else "lbfgs"

    log_reg = LogisticRegression(
        C=C,
        penalty=penalty,
        solver=solver,
        class_weight="balanced",  # ë¶ˆê· í˜• ëŒ€ì‘
        max_iter=1000,
        random_state=42
    )

    # SMOTE + ì „ì²˜ë¦¬ + ëª¨ë¸ íŒŒì´í”„ë¼ì¸
    clf = ImbPipeline(steps=[
        ("preprocessor", preprocessor),
        ("smote", SMOTE(sampling_strategy=0.2, random_state=42)),  # ë¶ˆëŸ‰=ì–‘í’ˆì˜ 20%
        ("model", log_reg)
    ])

    score = cross_val_score(
        clf, X, y,
        scoring=make_scorer(f1_score, pos_label=1),
        cv=cv,
        n_jobs=-1
    ).mean()

    return score

# -------------------------------
# 5. Optuna ì‹¤í–‰
# -------------------------------
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50, n_jobs=-1)

print("\nâœ… [Optuna] Best Params:", study.best_params)
print("âœ… [Optuna] Best CV F1 Score:", study.best_value)

# -------------------------------
# 6. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
# -------------------------------
best_params = study.best_params
solver = "liblinear" if best_params["penalty"] == "l1" else "lbfgs"

log_reg_best = LogisticRegression(
    C=best_params["C"],
    penalty=best_params["penalty"],
    solver=solver,
    class_weight="balanced",
    max_iter=1000,
    random_state=42
)

final_clf = ImbPipeline(steps=[
    ("preprocessor", preprocessor),
    ("smote", SMOTE(sampling_strategy=0.2, random_state=42)),
    ("model", log_reg_best)
])

train_x, test_x, train_y, test_y = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=0
)

final_clf.fit(train_x, train_y)

# -------------------------------
# 7. í‰ê°€
# -------------------------------
y_pred = final_clf.predict(test_x)
y_proba = final_clf.predict_proba(test_x)[:, 1]

print("\nğŸ“Š [Final Logistic Regression + SMOTE] Test Results")
print("Accuracy :", accuracy_score(test_y, y_pred))
print("F1 Score :", f1_score(test_y, y_pred, pos_label=1))
print("AUC      :", roc_auc_score(test_y, y_proba))
print("Recall(Sensitivity, ë¶ˆëŸ‰=1):", recall_score(test_y, y_pred, pos_label=1))
print("\nâœ… [Optuna] Best Params:", study.best_params)
