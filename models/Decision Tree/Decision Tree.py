import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score, make_scorer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import optuna

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("C:\\Users\\USER\\Desktop\\diecasting-project\\data\\data1.csv")
X = df.drop(columns="passorfail")
y = df["passorfail"]

# 2. ì»¬ëŸ¼ íƒ€ì… ë¶„ë¦¬
cat_cols = X.select_dtypes(include=["object"]).columns.tolist()
num_cols = X.select_dtypes(include=[np.number]).columns.tolist()

# 3. ì „ì²˜ë¦¬ê¸°
preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
        ("num", StandardScaler(), num_cols)
    ]
)

# 4. Stratified K-Fold
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 5. Optuna Objective í•¨ìˆ˜ ì •ì˜
def objective(trial):
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„ ì •ì˜
    params = {
        "criterion": trial.suggest_categorical("criterion", ["gini", "entropy", "log_loss"]),
        "max_depth": trial.suggest_int("max_depth", 2, 30),
        "min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
        "min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 20),
        "max_features": trial.suggest_categorical("max_features", ["sqrt", "log2", None]),
        "random_state": 42
    }

    # ëª¨ë¸ ì •ì˜
    dt_model = DecisionTreeClassifier(**params)

    # íŒŒì´í”„ë¼ì¸ (ì „ì²˜ë¦¬ + SMOTE + ëª¨ë¸)
    clf = ImbPipeline(steps=[
        ("preprocessor", preprocessor),
        ("smote", SMOTE(sampling_strategy=0.2, random_state=42)),
        ("model", dt_model)
    ])

    # êµì°¨ê²€ì¦ (ë¶ˆëŸ‰=1 ê¸°ì¤€ F1 ì ìˆ˜)
    score = cross_val_score(
        clf, X, y,
        scoring=make_scorer(f1_score, pos_label=1),
        cv=cv,
        n_jobs=-1
    ).mean()

    return score

# 6. Optuna ì‹¤í–‰
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)  # ì‹œë„ íšŸìˆ˜ ì¡°ì • ê°€ëŠ¥

print("\nâœ… Best Params:", study.best_params)
print("âœ… Best CV F1 Score:", study.best_value)

# 7. ìµœì  ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
best_params = study.best_params
best_model = DecisionTreeClassifier(**best_params)

clf_best = ImbPipeline(steps=[
    ("preprocessor", preprocessor),
    ("smote", SMOTE(sampling_strategy=0.2, random_state=42)),
    ("model", best_model)
])

# ë°ì´í„° ë¶„í• 
train_x, test_x, train_y, test_y = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=0
)

clf_best.fit(train_x, train_y)

# ì˜ˆì¸¡
from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, classification_report

y_pred = clf_best.predict(test_x)
y_proba = clf_best.predict_proba(test_x)[:, 1]

print("\nğŸ“Š Test Results with Decision Tree + SMOTE(20%) + Bayesian Optimization")
print("Accuracy :", accuracy_score(test_y, y_pred))
print("F1 Score :", f1_score(test_y, y_pred, pos_label=1))
print("Recall   :", recall_score(test_y, y_pred, pos_label=1))  # ë¯¼ê°ë„
print("AUC      :", roc_auc_score(test_y, y_proba))
print("\nClassification Report:\n", classification_report(test_y, y_pred, target_names=["ì–‘í’ˆ(0)", "ë¶ˆëŸ‰(1)"]))
